{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "from KerasRFCN.Model.Model import RFCN_Model\n",
    "from KerasRFCN.Config import Config\n",
    "from KerasRFCN.Utils import Dataset\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFCNNConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"Fashion\"\n",
    "\n",
    "    # Backbone model\n",
    "    # choose one from ['resnet50', 'resnet101', 'resnet50_dilated', 'resnet101_dilated']\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    C = 1 + 46  # background + 2 tags\n",
    "    NUM_CLASSES = C\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 640\n",
    "    IMAGE_MAX_DIM = 768\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)  # anchor side in pixels\n",
    "    # Use same strides on stage 4-6 if use dilated resnet of DetNet\n",
    "    # Like BACKBONE_STRIDES = [4, 8, 16, 16, 16]\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 200\n",
    "\n",
    "    RPN_NMS_THRESHOLD = 0.6\n",
    "    POOL_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDataset(Dataset):\n",
    "    # count - int, images in the dataset\n",
    "    def initDB(self, count, start = 0):\n",
    "        self.start = start\n",
    "\n",
    "        all_images, classes_count, class_mapping = pickle.load(open(\"data.pk\", \"rb\"))\n",
    "        self.classes = {}\n",
    "        # Add classes\n",
    "        for k,c in class_mapping.items():\n",
    "            self.add_class(\"Fashion\",c,k)\n",
    "            self.classes[c] = k\n",
    "\n",
    "        for k, item in enumerate(all_images[start:count+start]):\n",
    "            self.add_image(source=\"Fashion\",image_id=k, path=item['filepath'], width=item['width'], height=item['height'], bboxes=item['bboxes'])\n",
    "\n",
    "        self.rootpath = '/content/'\n",
    "\n",
    "    # read image from file and get the \n",
    "    def load_image(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        # tempImg = image.img_to_array( image.load_img(info['path']) )\n",
    "        tempImg = np.array(Image.open( os.path.join(self.rootpath, info['path']) ))\n",
    "        return tempImg\n",
    "\n",
    "    def get_keys(self, d, value):\n",
    "        return [k for k,v in d.items() if v == value]\n",
    "\n",
    "    def load_bbox(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for item in info['bboxes']:\n",
    "            bboxes.append((item['y1'], item['x1'], item['y2'], item['x2']))\n",
    "            label_key = self.get_keys(self.classes, item['class'])\n",
    "            if len(label_key) == 0:\n",
    "                continue\n",
    "            labels.extend( label_key )\n",
    "        return np.array(bboxes), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Current Working Directory\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Train Dataset\n",
    "config = RFCNNConfig()\n",
    "dataset_train = FashionDataset()\n",
    "dataset_train.initDB(100000)\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = FashionDataset()\n",
    "dataset_val.initDB(5000, start=100000)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "model = RFCN_Model(mode=\"training\", config=config, model_dir=os.path.join(ROOT_DIR, \"logs\") )\n",
    "\n",
    "# This is a hack, bacause the pre-train weights are not fit with dilated ResNet\n",
    "# model.keras_model.load_weights(\"resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\", by_name=True, skip_mismatch=True)\n",
    "\n",
    "try:\n",
    "    model_path = model.find_last()[1]\n",
    "    if model_path is not None:\n",
    "        model.load_weights(model_path, by_name=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"No checkpoint founded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training - Stage 1\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=20,\n",
    "            layers='heads')\n",
    "\n",
    "# Training - Stage 2\n",
    "# Finetune layers from ResNet stage 4 and up\n",
    "print(\"Fine tune Resnet stage 4 and up\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=40,\n",
    "            layers='4+')\n",
    "\n",
    "# Training - Stage 3\n",
    "# Fine tune all layers\n",
    "print(\"Fine tune all layers\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=80,\n",
    "            layers='all')\n",
    "\n",
    "# Training - Stage 3\n",
    "# Fine tune all layers\n",
    "print(\"Fine tune all layers\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=240,\n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
